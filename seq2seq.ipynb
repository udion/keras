{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train1, y_train1), (X_test1, y_test1) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=max_review_length)\n",
    "X_test1 = sequence.pad_sequences(X_test1, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps = 5\n",
    "input_dim = max_review_length\n",
    "latent_dim =8\n",
    "len(X_train1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation with simple seq2seq auto-encoder\n",
    "Here I pass the same sentence (seq of words) as input as well as target for the training, apart for testing syntax it doesn't really have any meaning, simply following what was shown in the diagram in the assignment sheet. A more interesting version of it would be input and target pairs which are at offset of 1(or someother number), target being ahead of input. We can then hope that the model will learn to predict wht usually comes after a given set of sequences. Not the main problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5, 500) (5000, 5, 500) (5000, 5, 500) (5000, 5, 500)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = X_train1.reshape(-1, 5, 500)\n",
    "X_test1 = X_test1.reshape(-1, 5, 500)\n",
    "y_train1 = X_train1\n",
    "y_test1 = X_test1\n",
    "print(X_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just encoder\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decoder part\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "sequence_autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_autoencoder.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 400/5000 [=>............................] - ETA: 2s - loss: 0.0116 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udion/Misc/anaconda3/envs/DeepCV/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 401us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 2/15\n",
      "5000/5000 [==============================] - 2s 391us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 3/15\n",
      "5000/5000 [==============================] - 2s 398us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 4/15\n",
      "5000/5000 [==============================] - 2s 393us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 5/15\n",
      "5000/5000 [==============================] - 2s 395us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 6/15\n",
      "5000/5000 [==============================] - 2s 399us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 7/15\n",
      "5000/5000 [==============================] - 2s 394us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 8/15\n",
      "5000/5000 [==============================] - 2s 394us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 9/15\n",
      "5000/5000 [==============================] - 2s 400us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 10/15\n",
      "5000/5000 [==============================] - 2s 400us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 11/15\n",
      "5000/5000 [==============================] - 2s 397us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 12/15\n",
      "5000/5000 [==============================] - 2s 401us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 13/15\n",
      "5000/5000 [==============================] - 2s 399us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 14/15\n",
      "5000/5000 [==============================] - 2s 412us/step - loss: 0.0114 - acc: 1.6000e-04\n",
      "Epoch 15/15\n",
      "5000/5000 [==============================] - 2s 397us/step - loss: 0.0114 - acc: 1.6000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f12f87c6c18>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_autoencoder.fit(X_train1, y_train1, batch_size=200, nb_epoch=15, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second part\n",
    "This part of the problem only uses the embeddings from the encoder and not anything after that, It isn't using the concept of decoder, hence it's equivalent to a model which has an encoder and a dense layer at top of that, \n",
    "as can be seen below, even with just 5 epoches it was possible to get an accuracy of 82.2% with this simple model.\n",
    "\n",
    "The dense layer has a single node as output with softmax, which performs the classification whether the input was a good review or bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 151s 6ms/step - loss: 0.4640 - acc: 0.7782 - val_loss: 0.4241 - val_acc: 0.8045\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 150s 6ms/step - loss: 0.2984 - acc: 0.8778 - val_loss: 0.3834 - val_acc: 0.8311\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.2165 - acc: 0.9155 - val_loss: 0.4141 - val_acc: 0.8243\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.1493 - acc: 0.9454 - val_loss: 0.4741 - val_acc: 0.8290\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.1075 - acc: 0.9612 - val_loss: 0.5732 - val_acc: 0.8220\n",
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "score :0.5732381032419205\n",
      "acc :0.822\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('score :{}'.format(score))\n",
    "print('acc :{}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
